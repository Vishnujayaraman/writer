{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c11df92b-04c0-44ff-83f2-2cd37fff0ca8",
   "metadata": {},
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64495b4b-2f57-438c-857a-d31eaef7a66c",
   "metadata": {},
   "source": [
    "# Implementation of Q learning in Python"
   ]
  },
  {
   "cell_type": "raw",
   "id": "523f4c5c-c42a-4d00-b0ba-0b7d4dbbed5a",
   "metadata": {},
   "source": [
    "Algorithm\n",
    "\n",
    "1. Initialize Environment: \n",
    "2. Create the OpenAI Gym environment (CartPole-v1 in this case). \n",
    "3. Initialize Q-Learning Agent: \n",
    "4. Define a QLearning Agent class with methods for choosing actions and updating the Q-table based on rewards. \n",
    "5. The Q-learning agent has parameters such as learning rate (alpha), discount factor (gamma), and exploration rate (epsilon). \n",
    "6. Initialize the Q-table with zeros. \n",
    "7. Training Loop: \n",
    "8. For a specified number of episodes: \n",
    "9. Reset the environment to the initial state. \n",
    "10. Initialize the total reward for the episode to zero. \n",
    "11. While the episode is not done: \n",
    "12. Choose an action using epsilon-greedy policy (with exploration rate epsilon). \n",
    "13. Take the chosen action and observe the next state and reward. \n",
    "14. Update the Q-table using the Q-learning update equation. \n",
    "15. Update the total reward for the episode. \n",
    "16. Transition to the next state. \n",
    "17. Print Progress: \n",
    "18. Optionally, print the total reward obtained in each episode to track the agent's progress. \n",
    "19. Close Environment: \n",
    "20. Close the environment after training is completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e074d13d-5dfd-46f3-a26e-15dc923f5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad278aa7-0203-4367-b890-f6f32eb614f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, num_state_bins, num_actions, state_bins, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.num_state_bins = num_state_bins  # Number of bins for state discretization\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.state_bins = state_bins  # Store state_bins as an attribute\n",
    "        self.q_table = np.zeros(tuple(self.num_state_bins) + (num_actions,))\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"\n",
    "        Discretize the continuous state into discrete bins.\n",
    "        \"\"\"\n",
    "        state_bins = []\n",
    "        for i in range(len(state)):  # Iterate over each of the state values\n",
    "            bin_idx = int(np.digitize(state[i], self.state_bins[i])) - 1  # Get the index of the bin\n",
    "            state_bins.append(bin_idx)  # Append the bin index to the list\n",
    "        return tuple(state_bins)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Choose an action using epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        if np.random.uniform(0, 1) < self.epsilon:  # Explore\n",
    "            return np.random.randint(0, self.num_actions)\n",
    "        else:  # Exploit\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Update the Q-table using the Q-learning update rule.\n",
    "        \"\"\"\n",
    "        best_next_action = np.argmax(self.q_table[next_state])  # Max Q value for next state\n",
    "        td_target = reward + self.gamma * self.q_table[next_state + (best_next_action,)]\n",
    "        td_error = td_target - self.q_table[state + (action,)]\n",
    "        self.q_table[state + (action,)] += self.alpha * td_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c3898-70c0-4a02-91eb-be5450b09c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e8665-27fd-4077-ae21-a976d8983c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state space discretization\n",
    "num_state_bins = [6, 6, 12, 12]  # Number of bins for each of the four state variables (position, velocity, angle, angular velocity)\n",
    "state_bins = [\n",
    "    np.linspace(-2.4, 2.4, num_state_bins[0] + 1),  # Position\n",
    "    np.linspace(-3.0, 3.0, num_state_bins[1] + 1),  # Velocity\n",
    "    np.linspace(-0.5, 0.5, num_state_bins[2] + 1),  # Angle\n",
    "    np.linspace(-2.0, 2.0, num_state_bins[3] + 1)   # Angular velocity\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377035c9-d06b-49d0-83c3-5dfc2d4b6518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Q-learning agent\n",
    "num_actions = env.action_space.n  # Number of possible actions (0 or 1)\n",
    "q_learning_agent = QLearningAgent(num_state_bins, num_actions, state_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33658060-c804-4f9b-95c5-6bdac8e1d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Q-learning agent\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = q_learning_agent.discretize_state(state)  # Discretize the initial state\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = q_learning_agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = q_learning_agent.discretize_state(next_state)  # Discretize next state\n",
    "        q_learning_agent.update_q_table(state, action, reward, next_state)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2969b4-4321-4b4d-9c73-bda240db11ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
